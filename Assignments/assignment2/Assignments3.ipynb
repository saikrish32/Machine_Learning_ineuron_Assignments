{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignments3.ipynb","provenance":[],"authorship_tag":"ABX9TyOnQ2LsiS6Sg8hzx6NGaf6w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1. Which Linear Regression algorithm we can use if we have a training set with\n","millions of features?\n","2. Can the Gradient Descent Algorithm get stuck in a local minimum when training a\n","linear regression model?\n","3. Do all Gradient Descent Algorithms lead to the same model if they are running for\n","the same no of epochs?\n","4. If you are doing a batch gradient descent and you are monitoring the validation\n","error at every epoch. If the validation error is constantly increasing what can be\n","the problem? How to fix that?"],"metadata":{"id":"j0VkyM2p2LzM"}},{"cell_type":"markdown","source":["## Answer1"],"metadata":{"id":"L18cSnPe3Di6"}},{"cell_type":"markdown","source":["Since there are lots of features, we cannot use Normal Equations (it will be very, very computationally expensive). Instead we can use Gradient Descent.\n","\n","You could use batch gradient descent, stochastic gradient descent, or mini-batch gradient descent. SGD and MBGD would work the best because neither of them need to load the entire dataset into memory in order to take 1 step of gradient descent. Batch would be ok if your processor is having enough memory to load all the data.\n","\n","The normal equations method would not be a good choice because it is computationally inefficient. The main cause of the computational complexity comes from inverse operation on an (n x n) matrix."],"metadata":{"id":"sb2kXtdG3HYr"}},{"cell_type":"markdown","source":["## Answer2"],"metadata":{"id":"6uIgnapJ3p1Q"}},{"cell_type":"markdown","source":["Here depending on the initial starting point, the Gradient Descent may end up stuck in local minima. In this case we can reset the initial value and do gradient descent again. However the Loss function for Linear Regression (MSE) is a convex function ie if we pick any two points in the curve, the line segment joining them never crosses the line.\n","\n","Gradient descent produces a convex shaped graph which only has one global optimum. Therefore, it cannot get stuck in a local minimum"],"metadata":{"id":"Htq3mk8T6drm"}},{"cell_type":"markdown","source":["## Answer3"],"metadata":{"id":"7TL0Mvnm7O7d"}},{"cell_type":"markdown","source":["No. The issue is that stochastic gradient descent and mini-batch gradient descent have randomness built into them. This means that they can find their way to nearby the global optimum, but they generally don't converge."],"metadata":{"id":"ncKbLTuq7ous"}},{"cell_type":"markdown","source":["## Answer4"],"metadata":{"id":"u6hqiyhM8H0w"}},{"cell_type":"markdown","source":["If the validation error consistently goes up after every epoch, then one possibility is that the learning rate is too high and the algorithm is diverging. If the training error also goes up, then this is clearly the problem and you should reduce the learning rate. However, if the training error is not going up, then your model is overfitting the training set and you should stop training and apply the common remedies to overfitting (regularization, more data, fix errors in data, remove outliers, or reduce number of features)."],"metadata":{"id":"kv2xHGZI8LjT"}},{"cell_type":"code","source":[""],"metadata":{"id":"NiBmXt8f2CTU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_iSXV1JK2CWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NNdl65TB2CZ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"iDle0SvS2Cdg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2a6FcnT82Cgw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"DTMMkVqJ2CkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"SPhOOSsp2Cnt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ESlmy2Le2Crh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"QTsDvsvR2Cvh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2Ei-jolM2CzL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"1lcZukw12C16"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Tgc1cPxW2C42"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"sI0zfDgS2C8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"aSbAl6zv2C_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"km7sKVCO2DB9"},"execution_count":null,"outputs":[]}]}